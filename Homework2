### Question 1
1.Write a HDFS program to divide a file in HDFS into N small files in the local file system.
The main program should take the name of the HDFS file and the number of local files N.
Divide the HDFS file equally in terms of size. One local file is allowed to have less bytes
than the rest of local files if the HDFS files can not be evenly divided. Hint: Use
FileSystem.getFileStatus.getLen() API to get the size of the HDFS file.
Hint: Use FileSystem.get(URI uri, Configuration conf)
Also check the following link:
http://hadoop.apache.org/docs/r1.0.4/api/org/apache/hadoop/fs/FileSystem.html
Hint: Use FSDataInputStream and FSDataOutputStream for reading and writing files.
You can get number of bytes for each file by dividing fileLength with divisions.
You need to create n files in local fileSystem where n is the number of division. You can
write bytes into generated files using FSDataOutputStream object.


import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.Path;
import java.io.IOException;
import java.net.URI;

public class ReadWrite {
    public static void main(String[] args) throws IOException {
    	 long numberSplit;
    	 long splitsize;
    	 long lastsplit;
    
      	Configuration conf = new Configuration();
   
       	FileSystem hdfs = FileSystem.get(URI.create("hdfs://quickstart.cloudera"), conf);
       	FileSystem ls = FileSystem.get(URI.create("file:///home/cloudera"), conf);
          Path path = new Path(args[0]);
          numberSplit=Integer.parseInt( args[1]);
          
           long length = hdfs.getFileStatus(path).getLen();
         
           splitsize = length/numberSplit;
           lastsplit = length%numberSplit;
           FSDataInputStream in = hdfs.open(path);
          
           
           byte buffer[] = new byte[(int) 256];
           
           int bytesRead = 0;
           
           bytesRead = in.read(buffer);
           
           for (int i=1;i<=numberSplit;i++)
           {   long count =0;
        	   Path outFile = new Path("Split_Files" + i);
               FSDataOutputStream out = ls.create(outFile);
               while ((bytesRead = in.read(buffer)) > 0) {
            	   count++;
            	   if ((count*256) <=splitsize)
            		   out.write(buffer, 0, bytesRead);
            	   else
            		   break;
            		   
               }
               out.close();
               
           }
            
      //  System.out.println("The Size of the file is " + length+ 
        //		"  Number of splits specified " + numberSplit + "  Splitsize " +splitsize +"  Last split size " + lastsplit + " " );
        
         }
}


### Question 3

3.Use Hadoop to do Median/Standard deviation ( all with combiner).
For Median/Standard deviation, Given a list of users' comments, determine the median
and standard deviation of comment lengths per day
Make sure the code working correctly in VM. Then export the executables from Eclipse.
Run the executables in the pseudo distributed mode
Hint: 1) Combiner will not be same as reducer as median/Standard Deviation is not
associative.
2) Creation date will be the key and comment length will be the value. You can get day
from CreationDate from Comments.xml.
3) Follow MapReduceDesign Pattern book, page no. 28 to 32.




###Two versions 

Version 1 :

package npu11117.wordcount;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.Map;
import java.util.HashMap;
import java.util.Map.Entry;

import npu11117.wordcount.MedianStdDevDriverWithCombiner.MedianStdDevCombiner;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.SortedMapWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Reducer.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

 class Utilities {
	
	public static final String[] REDIS_INSTANCES = { "p0", "p1", "p2", "p3",
			"p4", "p6" };

	// This helper function parses the stackoverflow into a Map for us.
	public static Map<String, String> transformXmlToMap(String xml) {
		Map<String, String> map = new HashMap<String, String>();
		try {
			String[] tokens = xml.trim().substring(5, xml.trim().length() - 3)
					.split("\"");

			for (int i = 0; i < tokens.length - 1; i += 2) {
				String key = tokens[i].trim();
				String val = tokens[i + 1];

				map.put(key.substring(0, key.length() - 1), val);
			}
		} catch (StringIndexOutOfBoundsException e) {
			System.err.println(xml);
		}

		return map;
	}
}

public class StdDeviationAndMedian {

	public static class MedianStdDevMapper extends
			Mapper<Object, Text, IntWritable, IntWritable> {

		private IntWritable outDate = new IntWritable();
		private IntWritable outCommentLength = new IntWritable();

		private final static SimpleDateFormat frmt = new SimpleDateFormat( "yyyy-MM-dd'T'HH:mm:ss.SSS");

		@SuppressWarnings("deprecation")
		@Override
		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {

			// Parse the input string into a nice map
			Map<String, String> parsed = Utilities.transformXmlToMap(value.toString());

			// Grab the "CreationDate" field,
			// since it is what we are grouping by
			String strDate = parsed.get("CreationDate");

			// Grab the comment to find the length
			String text = parsed.get("Text");

			// .get will return null if the key is not there
			if (strDate == null || text == null) {
				// skip this record
				return;
			}

			try {
				// get the hour this comment was posted in
				Date creationDate = frmt.parse(strDate);
				outDate.set(creationDate.getDate());

				// get the comment length
				outCommentLength.set(text.length());

				// write out the user ID with min max dates and count
				context.write(outDate, outCommentLength);

			} catch (ParseException e) {
				System.err.println(e.getMessage());
				return;
			}
		}
	}
	
	public static class MedianStdDevCombiner
	extends
	Reducer<IntWritable, SortedMapWritable, IntWritable, SortedMapWritable> {

@SuppressWarnings("rawtypes")
protected void reduce(IntWritable key,
		Iterable<SortedMapWritable> values, Context context)
		throws IOException, InterruptedException {

	SortedMapWritable outValue = new SortedMapWritable();

	for (SortedMapWritable v : values) {
		for (Entry<WritableComparable, Writable> entry : v.entrySet()) {
			LongWritable count = (LongWritable) outValue.get(entry
					.getKey());

			if (count != null) {
				count.set(count.get()
						+ ((LongWritable) entry.getValue()).get());
			} else {
				outValue.put(entry.getKey(), new LongWritable(
						((LongWritable) entry.getValue()).get()));
			}
		}
	}

	context.write(key, outValue);
}
}

	public static class MedianStdDevReducer extends
			Reducer<IntWritable, IntWritable, IntWritable, MedianStdDevTuple> {
		private MedianStdDevTuple result = new MedianStdDevTuple();
		private ArrayList<Float> commentLengths = new ArrayList<Float>();

		@Override
		public void reduce(IntWritable key, Iterable<IntWritable> values,
				Context context) throws IOException, InterruptedException {

			float sum = 0;
			float count = 0;
			commentLengths.clear();
			result.setStdDev(0);
			
			// Iterate through all input values for this key
			for (IntWritable val : values) {
				commentLengths.add((float) val.get());
				sum += val.get();
				++count;
			}

			// sort commentLengths to calculate median
			Collections.sort(commentLengths);

			// if commentLengths is an even value, average middle two elements
			if (count % 2 == 0) {
				result.setMedian((commentLengths.get((int) count / 2 - 1) + commentLengths
						.get((int) count / 2)) / 2.0f);
			} else {
				// else, set median to middle value
				result.setMedian(commentLengths.get((int) count / 2));
			}

			// calculate standard deviation
			float mean = sum / count;

			float sumOfSquares = 0.0f;
			for (Float f : commentLengths) {
				sumOfSquares += (f - mean) * (f - mean);
			}

			result.setStdDev((float) Math.sqrt(sumOfSquares / (count - 1)));

			context.write(key, result);
		}
	}

	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		String[] otherArgs = new GenericOptionsParser(conf, args)
				.getRemainingArgs();
		if (otherArgs.length != 2) {
			System.err.println("Usage: MedianStdDevDriver <in> <out>");
			System.exit(2);
		}
		Job job = new Job(conf,
				"StackOverflow Comment Length Median StdDev By Hour");
		job.setJarByClass(StdDeviationAndMedian.class);
		job.setMapperClass(MedianStdDevMapper.class);
		job.setCombinerClass(MedianStdDevCombiner.class);
		job.setReducerClass(MedianStdDevReducer.class);
		job.setMapOutputKeyClass(IntWritable.class);
		job.setMapOutputValueClass(IntWritable.class);
		job.setOutputKeyClass(IntWritable.class);
		job.setOutputValueClass(MedianStdDevTuple.class);
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}

	public static class MedianStdDevTuple implements Writable {
		private float median = 0;
		private float stddev = 0f;

		public float getMedian() {
			return median;
		}

		public void setMedian(float median) {
			this.median = median;
		}

		public float getStdDev() {
			return stddev;
		}

		public void setStdDev(float stddev) {
			this.stddev = stddev;
		}

		@Override
		public void readFields(DataInput in) throws IOException {
			median = in.readFloat();
			stddev = in.readFloat();
		}

		@Override
		public void write(DataOutput out) throws IOException {
			out.writeFloat(median);
			out.writeFloat(stddev);
		}

		@Override
		public String toString() {
			return median + "\t" + stddev;
		}
	}
}


















package npu11117.wordcount;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.Map;
import java.util.HashMap;
import java.util.Map.Entry;

import npu11117.wordcount.MedianStdDevDriverWithCombiner.MedianStdDevCombiner;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.SortedMapWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Reducer.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

 class Utilities {
	
	public static final String[] REDIS_INSTANCES = { "p0", "p1", "p2", "p3",
			"p4", "p6" };

	// This helper function parses the stackoverflow into a Map for us.
	public static Map<String, String> transformXmlToMap(String xml) {
		Map<String, String> map = new HashMap<String, String>();
		try {
			String[] tokens = xml.trim().substring(5, xml.trim().length() - 3)
					.split("\"");

			for (int i = 0; i < tokens.length - 1; i += 2) {
				String key = tokens[i].trim();
				String val = tokens[i + 1];

				map.put(key.substring(0, key.length() - 1), val);
			}
		} catch (StringIndexOutOfBoundsException e) {
			System.err.println(xml);
		}

		return map;
	}
}

public class StdDeviationAndMedian {

	public static class MedianStdDevMapper extends
			Mapper<Object, Text, IntWritable, IntWritable> {

		private IntWritable outDate = new IntWritable();
		private IntWritable outCommentLength = new IntWritable();

		private final static SimpleDateFormat frmt = new SimpleDateFormat( "yyyy-MM-dd'T'HH:mm:ss.SSS");

		@SuppressWarnings("deprecation")
		@Override
		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {

			// Parse the input string into a nice map
			Map<String, String> parsed = Utilities.transformXmlToMap(value.toString());

			// Grab the "CreationDate" field,
			// since it is what we are grouping by
			String strDate = parsed.get("CreationDate");

			// Grab the comment to find the length
			String text = parsed.get("Text");

			// .get will return null if the key is not there
			if (strDate == null || text == null) {
				// skip this record
				return;
			}

			try {
				// get the hour this comment was posted in
				Date creationDate = frmt.parse(strDate);
				outDate.set(creationDate.getDate());

				// get the comment length
				outCommentLength.set(text.length());

				// write out the user ID with min max dates and count
				context.write(outDate, outCommentLength);

			} catch (ParseException e) {
				System.err.println(e.getMessage());
				return;
			}
		}
	}
	
	public static class MedianStdDevCombiner
	extends
	Reducer<IntWritable, SortedMapWritable, IntWritable, SortedMapWritable> {

@SuppressWarnings("rawtypes")
protected void reduce(IntWritable key,
		Iterable<SortedMapWritable> values, Context context)
		throws IOException, InterruptedException {

	SortedMapWritable outValue = new SortedMapWritable();

	for (SortedMapWritable v : values) {
		for (Entry<WritableComparable, Writable> entry : v.entrySet()) {
			LongWritable count = (LongWritable) outValue.get(entry
					.getKey());

			if (count != null) {
				count.set(count.get()
						+ ((LongWritable) entry.getValue()).get());
			} else {
				outValue.put(entry.getKey(), new LongWritable(
						((LongWritable) entry.getValue()).get()));
			}
		}
	}

	context.write(key, outValue);
}
}

	public static class MedianStdDevReducer extends
			Reducer<IntWritable, IntWritable, IntWritable, MedianStdDevTuple> {
		private MedianStdDevTuple result = new MedianStdDevTuple();
		private ArrayList<Float> commentLengths = new ArrayList<Float>();

		@Override
		public void reduce(IntWritable key, Iterable<IntWritable> values,
				Context context) throws IOException, InterruptedException {

			float sum = 0;
			float count = 0;
			commentLengths.clear();
			result.setStdDev(0);
			
			// Iterate through all input values for this key
			for (IntWritable val : values) {
				commentLengths.add((float) val.get());
				sum += val.get();
				++count;
			}

			// sort commentLengths to calculate median
			Collections.sort(commentLengths);

			// if commentLengths is an even value, average middle two elements
			if (count % 2 == 0) {
				result.setMedian((commentLengths.get((int) count / 2 - 1) + commentLengths
						.get((int) count / 2)) / 2.0f);
			} else {
				// else, set median to middle value
				result.setMedian(commentLengths.get((int) count / 2));
			}

			// calculate standard deviation
			float mean = sum / count;

			float sumOfSquares = 0.0f;
			for (Float f : commentLengths) {
				sumOfSquares += (f - mean) * (f - mean);
			}

			result.setStdDev((float) Math.sqrt(sumOfSquares / (count - 1)));

			context.write(key, result);
		}
	}

	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		String[] otherArgs = new GenericOptionsParser(conf, args)
				.getRemainingArgs();
		if (otherArgs.length != 2) {
			System.err.println("Usage: MedianStdDevDriver <in> <out>");
			System.exit(2);
		}
		Job job = new Job(conf,
				"StackOverflow Comment Length Median StdDev By Hour");
		job.setJarByClass(StdDeviationAndMedian.class);
		job.setMapperClass(MedianStdDevMapper.class);
		job.setCombinerClass(MedianStdDevCombiner.class);
		job.setReducerClass(MedianStdDevReducer.class);
		job.setMapOutputKeyClass(IntWritable.class);
		job.setMapOutputValueClass(IntWritable.class);
		job.setOutputKeyClass(IntWritable.class);
		job.setOutputValueClass(MedianStdDevTuple.class);
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}

	public static class MedianStdDevTuple implements Writable {
		private float median = 0;
		private float stddev = 0f;

		public float getMedian() {
			return median;
		}

		public void setMedian(float median) {
			this.median = median;
		}

		public float getStdDev() {
			return stddev;
		}

		public void setStdDev(float stddev) {
			this.stddev = stddev;
		}

		@Override
		public void readFields(DataInput in) throws IOException {
			median = in.readFloat();
			stddev = in.readFloat();
		}

		@Override
		public void write(DataOutput out) throws IOException {
			out.writeFloat(median);
			out.writeFloat(stddev);
		}

		@Override
		public String toString() {
			return median + "\t" + stddev;
		}
	}
}







Version 2:

package npu11117.wordcount;
import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.Map;
import java.util.TreeMap;
import java.util.Map.Entry;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.SortedMapWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.io.WritableComparable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

class Utilities1 {
	
	public static final String[] REDIS_INSTANCES = { "p0", "p1", "p2", "p3",
			"p4", "p6" };

	// This helper function parses the stackoverflow into a Map for us.
	public static Map<String, String> transformXmlToMap(String xml) {
		Map<String, String> map = new HashMap<String, String>();
		try {
			String[] tokens = xml.trim().substring(5, xml.trim().length() - 3)
					.split("\"");

			for (int i = 0; i < tokens.length - 1; i += 2) {
				String key = tokens[i].trim();
				String val = tokens[i + 1];

				map.put(key.substring(0, key.length() - 1), val);
			}
		} catch (StringIndexOutOfBoundsException e) {
			System.err.println(xml);
		}

		return map;
	}
}

public class MedianStdDevDriverWithCombiner {
	
	

	public static class MedianStdDevMapper extends
			Mapper<Object, Text, IntWritable, SortedMapWritable> {

		private IntWritable commentLength = new IntWritable();
		private static final LongWritable ONE = new LongWritable(1);
		private IntWritable outDate = new IntWritable();;

		private final static SimpleDateFormat frmt = new SimpleDateFormat(
				"yyyy-MM-dd'T'HH:mm:ss.SSS");

		@SuppressWarnings("deprecation")
		@Override
		public void map(Object key, Text value, Context context)
				throws IOException, InterruptedException {

			// Parse the input string into a nice map
			Map<String, String> parsed = Utilities1.transformXmlToMap(value
					.toString());

			// Grab the "CreationDate" field,
			// since it is what we are grouping by
			String strDate = parsed.get("CreationDate");

			// Grab the comment to find the length
			String text = parsed.get("Text");

			// .get will return null if the key is not there
			if (strDate == null || text == null) {
				// skip this record
				return;
			}

			try {
				// get the hour this comment was posted in
				Date creationDate = frmt.parse(strDate);
		     	outDate.set(creationDate.getDate());
		
				commentLength.set(text.length());
				SortedMapWritable outCommentLength = new SortedMapWritable();
				outCommentLength.put(commentLength, ONE);

				// write out the user ID with min max dates and count
				context.write(outDate, outCommentLength);

			} catch (ParseException e) {
				System.err.println(e.getMessage());
				return;
			}
		}
	}

	public static class MedianStdDevCombiner
			extends
			Reducer<IntWritable, SortedMapWritable, IntWritable, SortedMapWritable> {

		@SuppressWarnings("rawtypes")
		protected void reduce(IntWritable key,
				Iterable<SortedMapWritable> values, Context context)
				throws IOException, InterruptedException {

			SortedMapWritable outValue = new SortedMapWritable();

			for (SortedMapWritable v : values) {
				for (Entry<WritableComparable, Writable> entry : v.entrySet()) {
					LongWritable count = (LongWritable) outValue.get(entry
							.getKey());

					if (count != null) {
						count.set(count.get()
								+ ((LongWritable) entry.getValue()).get());
					} else {
						outValue.put(entry.getKey(), new LongWritable(
								((LongWritable) entry.getValue()).get()));
					}
				}
			}

			context.write(key, outValue);
		}
	}

	public static class MedianStdDevReducer
			extends
			Reducer<IntWritable, SortedMapWritable, IntWritable, MedianStdDevTuple> {
		private MedianStdDevTuple result = new MedianStdDevTuple();
		private TreeMap<Integer, Long> commentLengthCounts = new TreeMap<Integer, Long>();

		@SuppressWarnings("rawtypes")
		@Override
		public void reduce(IntWritable key, Iterable<SortedMapWritable> values,
				Context context) throws IOException, InterruptedException {

			float sum = 0;
			long totalComments = 0;
			commentLengthCounts.clear();
			result.setMedian(0);
			result.setStdDev(0);

			for (SortedMapWritable v : values) {
				for (Entry<WritableComparable, Writable> entry : v.entrySet()) {
					int length = ((IntWritable) entry.getKey()).get();
					long count = ((LongWritable) entry.getValue()).get();

					totalComments += count;
					sum += length * count;

					Long storedCount = commentLengthCounts.get(length);
					if (storedCount == null) {
						commentLengthCounts.put(length, count);
					} else {
						commentLengthCounts.put(length, storedCount + count);
					}
				}
			}

			long medianIndex = totalComments / 2L;
			long previousComments = 0;
			long comments = 0;
			int prevKey = 0;
			for (Entry<Integer, Long> entry : commentLengthCounts.entrySet()) {
				comments = previousComments + entry.getValue();
				if (previousComments <= medianIndex && medianIndex < comments) {
					if (totalComments % 2 == 0) {
						if (previousComments == medianIndex) {
							result.setMedian((float) (entry.getKey() + prevKey) / 2.0f);
						} else {
							result.setMedian(entry.getKey());
						}
					} else {
						result.setMedian(entry.getKey());
					}
					break;
				}
				previousComments = comments;
				prevKey = entry.getKey();
			}

			// calculate standard deviation
			float mean = sum / totalComments;

			float sumOfSquares = 0.0f;
			for (Entry<Integer, Long> entry : commentLengthCounts.entrySet()) {
				sumOfSquares += (entry.getKey() - mean)
						* (entry.getKey() - mean) * entry.getValue();
			}

			result.setStdDev((float) Math.sqrt(sumOfSquares
					/ (totalComments - 1)));

			context.write(key, result);
		}
	}

	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		String[] otherArgs = new GenericOptionsParser(conf, args)
				.getRemainingArgs();
		if (otherArgs.length != 2) {
			System.err.println("Usage: MedianStdDevDriverWithCombiner <in> <out>");
			System.exit(2);
		}
		Job job = new Job(conf,
				"StackOverflow Comment Length Median StdDev By Hour");
		job.setJarByClass(MedianStdDevDriverWithCombiner.class);
		job.setMapperClass(MedianStdDevMapper.class);
		job.setCombinerClass(MedianStdDevCombiner.class);
		job.setReducerClass(MedianStdDevReducer.class);
		job.setMapOutputKeyClass(IntWritable.class);
		job.setMapOutputValueClass(SortedMapWritable.class);
		job.setOutputKeyClass(IntWritable.class);
		job.setOutputValueClass(MedianStdDevTuple.class);
		FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}

	public static class MedianStdDevTuple implements Writable {
		private float median = 0;
		private float stddev = 0f;

		public float getMedian() {
			return median;
		}

		public void setMedian(float median) {
			this.median = median;
		}

		public float getStdDev() {
			return stddev;
		}

		public void setStdDev(float stddev) {
			this.stddev = stddev;
		}

		@Override
		public void readFields(DataInput in) throws IOException {
			median = in.readFloat();
			stddev = in.readFloat();
		}

		@Override
		public void write(DataOutput out) throws IOException {
			out.writeFloat(median);
			out.writeFloat(stddev);
		}

		@Override
		public String toString() {
			return median + "\t" + stddev;
		}
	}
}





